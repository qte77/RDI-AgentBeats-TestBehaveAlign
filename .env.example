# TestBehaveAlign Environment Variables
# Copy this file to .env and fill in your values

# ============================================================================
# QUICK START
# ============================================================================
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Set GREEN_OPENAI_API_KEY (required for LLM-based evaluation):
#    GREEN_OPENAI_API_KEY=sk-your-actual-key-here
#
# 3. Run agents:
#    docker-compose up -d
#    OR
#    python -m bulletproof_green.server  # Port 8000
#    python -m bulletproof_purple.server # Port 8001
#
# 4. Debug settings:
#    python -m bulletproof_green.settings
#    python -m bulletproof_purple.settings
#
# All settings have sensible defaults. Only GREEN_OPENAI_API_KEY is required
# for LLM features (system gracefully falls back to rule-only scoring without it).

# ============================================================================
# GREEN AGENT CONFIGURATION
# ============================================================================

# Server settings
GREEN_PORT=8000
GREEN_HOST=0.0.0.0
GREEN_TIMEOUT=300

# Purple Agent connection (for arena mode)
GREEN_PURPLE_AGENT_URL=http://localhost:8001

# Agent card settings
GREEN_AGENT_CARD_TIMEOUT=30
GREEN_AGENT_CARD_CACHE_TTL=300

# LLM Judge settings (for hybrid scoring)
# IMPORTANT: Required for LLM-based evaluation features
GREEN_OPENAI_API_KEY=sk-your-key-here
GREEN_LLM_MODEL=gpt-4
GREEN_LLM_TEMPERATURE=0.0
GREEN_LLM_ALPHA=0.7
GREEN_LLM_BETA=0.3
GREEN_LLM_TIMEOUT=30.0

# Arena mode settings
GREEN_ARENA_MAX_ITERATIONS=5
GREEN_ARENA_TARGET_RISK_SCORE=20

# ============================================================================
# PURPLE AGENT CONFIGURATION
# ============================================================================

# Server settings
PURPLE_PORT=8001
PURPLE_HOST=0.0.0.0
PURPLE_TIMEOUT=300

# ============================================================================
# USAGE NOTES
# ============================================================================
#
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Update GREEN_OPENAI_API_KEY with your actual OpenAI API key
#
# 3. Adjust ports if running multiple instances or if ports are in use
#
# 4. For production, review and adjust timeout values based on your needs
#
# 5. LLM Judge (hybrid scoring):
#    - If no API key is provided, system falls back to rule-only scoring
#    - Alpha (0.7) = weight for rule-based score
#    - Beta (0.3) = weight for LLM score
#    - Final score = alpha * rule_score + beta * llm_score
