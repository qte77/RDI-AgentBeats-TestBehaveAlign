# AgentBeats Scenario Configuration
# Based on: https://github.com/RDI-Foundation/agentbeats-leaderboard-template
#
# TestBehaveAlign - Test Quality Benchmark for AI Coding Agents
# Green Agent (Evaluator): Measures test quality via fault detection and mutation testing
# Purple Agent (Baseline): Generates pytest/pytest-bdd tests from specifications
#
# Registration Guide: docs/AgentBeats/SUBMISSION-GUIDE.md
#
# IMPORTANT: Replace placeholder agentbeats_id values with actual IDs
# obtained from registering your agents at https://agentbeats.dev
#
# Docker Images (after GHCR push):
#   - ghcr.io/<org>/testbehavealign-green:latest
#   - ghcr.io/<org>/testbehavealign-purple:latest

[green_agent]
# Green agent: Test Quality Evaluator
# Runs generated tests against correct/buggy implementations
# Calculates: fault_detection_rate, mutation_score
# Register at agentbeats.dev and replace with actual ID
agentbeats_id = "testbehavealign-green"
env = { LOG_LEVEL = "INFO" }

[[participants]]
# Purple agent: Baseline Test Generator
# Generates pytest (TDD) or pytest-bdd (BDD) tests from specifications
# Register at agentbeats.dev and replace with actual ID
agentbeats_id = "testbehavealign-purple"
name = "purple"
env = { LOG_LEVEL = "INFO", OPENAI_API_KEY = "${OPENAI_API_KEY}" }

[config]
# Assessment parameters for test quality evaluation
track = "tdd"           # "tdd" or "bdd"
task_count = 5          # Number of tasks to evaluate (HumanEval 0-4)
timeout_per_task = 60   # Seconds per task evaluation
